{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "367c32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "pd.options.display.max_colwidth = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "07f79b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82103\\anaconda3\\envs\\DL\\lib\\site-packages\\openpyxl\\styles\\stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_excel('빅카인즈_뉴스.xlsx')\n",
    "df=df.loc[:,['일자','제목','본문']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74fc16f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16846, 3)\n"
     ]
    }
   ],
   "source": [
    "#중복 제거\n",
    "kr_news = df.drop_duplicates(subset=['본문'],keep='first').reset_index(drop=True)\n",
    "print(kr_news.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e0351a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트를 위해 3000개만..\n",
    "\n",
    "kr_df=kr_news.iloc[:3000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e70756a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "JVM cannot be initialized more than once.Please call koalanlp.Util.finalize() when you want to re-init the JVM with other options.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_15640\\2261685257.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkoalanlp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAPI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOKT\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'LATEST'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#: HNN=2.0.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msplitter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentenceSplitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAPI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOKT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\koalanlp\\Util.py\u001b[0m in \u001b[0;36minitialize\u001b[1;34m(java_options, lib_path, force_download, port, **packages)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"JVM initialization procedure is completed.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         raise Exception(\"JVM cannot be initialized more than once.\"\n\u001b[0m\u001b[0;32m    200\u001b[0m                         \"Please call koalanlp.Util.finalize() when you want to re-init the JVM with other options.\")\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: JVM cannot be initialized more than once.Please call koalanlp.Util.finalize() when you want to re-init the JVM with other options."
     ]
    }
   ],
   "source": [
    "#문장 단위로 짜르기 위함.\n",
    "from koalanlp.Util import initialize, finalize\n",
    "from koalanlp.proc import SentenceSplitter\n",
    "from koalanlp import API\n",
    "\n",
    "initialize(OKT='LATEST')  #: HNN=2.0.3\n",
    "\n",
    "splitter = SentenceSplitter(API.OKT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4b8cd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 아시아경제 손선희 기자 지난해 부진한 실적을 기록한 현대백화점에 대해 증권사들이 줄줄이 목표주가를 낮춰 잡았다 한국투자증권은 14일 현대백화점에 대한 목표주가를 기존 9만원에서 8만원으로 하향 조정한다고 밝혔다 현대백화점은 지난해 4분기 연결기준 영업이익 686억원을 기록했다 전년 동기 대비 27 2 줄었을 뿐만 아니라 시장 기대치에도 '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr_df.본문[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9878f5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8beed56ccb8847768c20f6107dc0c1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from tqdm.auto import tqdm\n",
    "sentence_list=[]\n",
    "index_list=[]\n",
    "for i in tqdm(range(kr_df.shape[0])):\n",
    "    s_text=splitter(kr_df.본문[i])\n",
    "    sentence_list.append(s_text)\n",
    "    count=len(s_text)\n",
    "    index_list.append(count)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b7c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1dafe8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraForTokenClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(54343, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "MODEL_NAME = \"beomi/KcELECTRA-base-v2022\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tag2id = {'B-ORG': 0, 'O': 1, 'I-ORG': 2}\n",
    "unique_tags={'B-ORG', 'I-ORG', 'O'}\n",
    "id2tag={0: 'B-ORG', 1: 'O', 2: 'I-ORG'}\n",
    "pad_token_id = tokenizer.pad_token_id # 0\n",
    "cls_token_id = tokenizer.cls_token_id # 101\n",
    "sep_token_id = tokenizer.sep_token_id # 102\n",
    "pad_token_label_id = tag2id['O']    # tag2id['O']\n",
    "cls_token_label_id = tag2id['O']\n",
    "sep_token_label_id = tag2id['O']\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('kcelectra_base_new', num_labels=len(unique_tags))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1ad46194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 토크나이저는 wordPiece tokenizer로 tokenizing 결과를 반환합니다.\n",
    "# 데이터 단위를 음절 단위로 변경했기 때문에, tokenizer도 음절 tokenizer로 변경\n",
    "\n",
    "# berttokenizer를 사용하는데 한국어 vocab이 8000개 정도 밖에 없고 그 안의 한국어들의 거의 음절로 존재\n",
    "# -> 음절 단위 tokenizer를 적용하면 vocab id를 어느 정도 획득할 수 있어 UNK가 별로 없을듯 하다\n",
    "def ner_tokenizer(sent, max_seq_length):    \n",
    "    pre_syllable = \"_\"\n",
    "    input_ids = [pad_token_id] * (max_seq_length - 1)\n",
    "    attention_mask = [0] * (max_seq_length - 1)\n",
    "    token_type_ids = [0] * max_seq_length\n",
    "    sent = sent[:max_seq_length-2]\n",
    "\n",
    "    for i, syllable in enumerate(sent):\n",
    "        if syllable == '_':\n",
    "            pre_syllable = syllable\n",
    "        if pre_syllable != \"_\":\n",
    "            syllable = '##' + syllable  # 중간 음절에는 모두 prefix를 붙입니다.\n",
    "            # 우리가 구성한 학습 데이터도 이렇게 구성되었기 때문이라고 함.\n",
    "            # 이순신은 조선 -> [이, ##순, ##신, ##은, 조, ##선]\n",
    "        pre_syllable = syllable\n",
    "\n",
    "        input_ids[i] = (tokenizer.convert_tokens_to_ids(syllable))\n",
    "        attention_mask[i] = 1\n",
    "    \n",
    "    input_ids = [cls_token_id] + input_ids\n",
    "    input_ids[len(sent)+1] = sep_token_id\n",
    "    attention_mask = [1] + attention_mask\n",
    "    attention_mask[len(sent)+1] = 1\n",
    "    return {\"input_ids\":input_ids,\n",
    "            \"attention_mask\":attention_mask,\n",
    "            \"token_type_ids\":token_type_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "998a0915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우리가 전에 사용했던건 word piece tokenizer\n",
    "# 지금 사용한건 음절단위 tokenizer\n",
    "# 반드시 음절 tokenizer를 거친 후에 model에 들어가야 한다.\n",
    "\n",
    "def ner_inference(text) : \n",
    "    words = []\n",
    "    tags= []\n",
    "    model.eval()\n",
    "    text = text.replace(' ', '_')\n",
    "\n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    tokenized_sent = ner_tokenizer(text, len(text)+2)\n",
    "    input_ids = torch.tensor(tokenized_sent['input_ids']).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(tokenized_sent['attention_mask']).unsqueeze(0).to(device)\n",
    "    token_type_ids = torch.tensor(tokenized_sent['token_type_ids']).unsqueeze(0).to(device)    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids)\n",
    "        \n",
    "    logits = outputs['logits']\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = token_type_ids.cpu().numpy()\n",
    "\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "    pred_tags = [list(tag2id.keys())[p_i] for p in predictions for p_i in p]\n",
    "\n",
    "    for i, tag in enumerate(pred_tags):\n",
    "      words.append(tokenizer.convert_ids_to_tokens(tokenized_sent['input_ids'][i]))\n",
    "      tags.append(tag)\n",
    "    return words, tags\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6892cb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b3660cb6e9478d8795432e8ecfab1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 사용 환경에 맞게 후 처리\n",
    "index_list=[]\n",
    "text_list= []\n",
    "outputs_list=[]\n",
    "tags_list =[]\n",
    "\n",
    "for i in tqdm(range(len(sentence_list))):\n",
    "    for text in sentence_list[i]:\n",
    "        words,tags = ner_inference(text)\n",
    "        index_list.append(i)\n",
    "\n",
    "        text_list.append(text)\n",
    "        tags_list.append(tags)\n",
    "        outputs_list.append(words)\n",
    "\n",
    "df=pd.DataFrame(zip(index_list,text_list,outputs_list,tags_list))\n",
    "df.columns=['news_index','text','output','tag']\n",
    "\n",
    "sum_list=[]\n",
    "for i in range(df.news_index.nunique()):\n",
    "    s_text=''.join(df[df.loc[:,'news_index']==i].text.to_list())\n",
    "    count = sum(df.loc[:,'news_index']==i)\n",
    "    for j in range(count):\n",
    "        sum_list.append(s_text)\n",
    "df['sum_text']=sum_list\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2d488ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "\n",
    "def export(test_tag, test_text):\n",
    "\n",
    "    # b index list\n",
    "    b_list = [i for i, x in enumerate(test_tag) if x == \"B-ORG\"]\n",
    "\n",
    "    # 임시 i list (나중에 b list랑 더함)\n",
    "    ii_list = []\n",
    "    for n in range(len(b_list)):\n",
    "        if n + 1 < len(b_list):\n",
    "            sample = test_tag[(b_list[n]):(b_list[n+1])]\n",
    "        else:\n",
    "            sample = test_tag[(b_list[n]):]\n",
    "\n",
    "        sample.reverse()\n",
    "\n",
    "        if \"I-ORG\" in sample:\n",
    "            ii_list.append(len(sample) - sample.index(\"I-ORG\"))\n",
    "        else:\n",
    "            ii_list.append(0)\n",
    "\n",
    "\n",
    "    # i list만들기 (임시 i list + b list)\n",
    "    i_list = []\n",
    "    for i, j in zip(b_list, ii_list):\n",
    "        i_list.append(i + j)\n",
    "\n",
    "\n",
    "    # b, i 매칭 -> 튜플로\n",
    "    ticker_range = list(zip(b_list, i_list))\n",
    "\n",
    "    # 티커 리스트\n",
    "    ticker_list = []\n",
    "\n",
    "    for r in ticker_range:\n",
    "        ticker_list.append(test_text[r[0]-1:r[1]-1])\n",
    "\n",
    "    return ticker_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aff6bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"기업명\"] = df.apply(lambda x: export(x.tag, x.text), axis=1)\n",
    "df.기업명=df.기업명.apply(lambda x : ','.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92d9a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('한국뉴스ner_모델.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "012bb9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_comp_list=[]\n",
    "\n",
    "\n",
    "# for i in range(len(df)):\n",
    "#   lst = df['tag'][i]\n",
    "#   b = np.array(lst)\n",
    "#   end = 0\n",
    "#   comp_list=[]\n",
    "#   for j in range(len(np.where(b == 'B-ORG')[0])):\n",
    "#     start = np.where(b == 'B-ORG')[0][j] - 1\n",
    "#     end_list = list(np.where(b == 'I-ORG')[0])\n",
    "#     end_list.append(999)\n",
    "    \n",
    "#     for k in range(end+1, len(end_list)):\n",
    "#         if end_list[k] - end_list[k-1] != 1:\n",
    "#             end = k\n",
    "#             break\n",
    "#         else:\n",
    "#             continue\n",
    "#     comp=df['text'][i][start:end_list[end-1]]\n",
    "#     comp_list.append(comp)\n",
    "#   last_comp_list.append(comp_list)\n",
    "\n",
    "# df['tickers']=last_comp_list\n",
    "# ################################\n",
    "# def tag_overlap(tag,indices):\n",
    "#     stacks = []\n",
    "#     for idx, oov in enumerate(indices['O']):\n",
    "#         if idx < len(indices['O'])-1:\n",
    "#             # Not B-ORG -> stack push\n",
    "#             if (indices['O'][idx+1] - indices['O'][idx] == 1 or tag[indices['O'][idx]+1] == 'I-ORG'):\n",
    "#                 stacks.append(oov)\n",
    "#             # Begin -> reset\n",
    "#             if tag[indices['O'][idx]+1] == 'B-ORG':\n",
    "#                 stacks = []\n",
    "#             # Abnormal I-ORG reached -> OOV to I-ORG\n",
    "#             elif tag[indices['O'][idx]+1] == 'I-ORG':\n",
    "#                 indices['I-ORG'].extend(stacks)\n",
    "#                 stacks = []\n",
    "#     indices['I-ORG'] = sorted(indices['I-ORG'])\n",
    "    \n",
    "#     return indices\n",
    "# #############################################\n",
    "# comp_list=[]\n",
    "# for i in tqdm(range(len(df))):\n",
    "# # i = 1314\n",
    "#     tickers = []\n",
    "\n",
    "#     lst = df['tag'][i]\n",
    "\n",
    "#     # Initialize dict of indices\n",
    "#     indices = dict.fromkeys(lst,[])\n",
    "#     indices = {tag:list(np.where(np.array(lst)==tag)[0]) for tag in indices}\n",
    "\n",
    "\n",
    "#     try:\n",
    "#         # Tag operation\n",
    "#         indices = tag_overlap(lst,indices)\n",
    "#         start_idx = indices['B-ORG']\n",
    "#         end_idx = indices['I-ORG']\n",
    "#         end_start_diff = [end_idx[idx]-end_idx[idx-1] if idx>0 else 0 for idx in range(len(end_idx))]\n",
    "\n",
    "# #         print(indices)\n",
    "# #         print(end_start_diff)\n",
    "\n",
    "\n",
    "\n",
    "#         #     diff_idx = [diff for diff in end_start_diff if diff != 1][:len(start_idx)]\n",
    "#         diff_idx = np.where(np.array(end_start_diff)!=1)[0][:len(start_idx)]\n",
    "\n",
    "#         indexes = [(start_idx[idx]-1,start_idx[idx]+(diff_idx[idx+1]-diff_idx[idx])) if idx < len(diff_idx)-1 else (start_idx[idx]-1,end_idx[-1]) for idx in range(len(diff_idx))]\n",
    "\n",
    "#         tickers = [df['text'][i][tup[0]:tup[1]] for tup in indexes]\n",
    "\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "#     comp_list.append(tickers)\n",
    "\n",
    "# #     start_idx = np.where(np.array(lst) == 'B-ORG')[0]\n",
    "\n",
    "# #     if len(start_idx) == 0:\n",
    "# #         comp_list.append(tickers)\n",
    "# #         continue\n",
    "\n",
    "# #     end_idx = np.where(np.array(lst) == 'I-ORG')[0]\n",
    "\n",
    "# #     end_start_diff = [end_idx[idx]-end_idx[idx-1] if idx>0 else 0 for idx in range(len(end_idx))]\n",
    "# # #     diff_idx = [diff for diff in end_start_diff if diff != 1][:len(start_idx)]\n",
    "# #     diff_idx = np.where(np.array(end_start_diff)!=1)[0][:len(start_idx)]\n",
    "\n",
    "# #     indexes = [(start_idx[idx]-1,start_idx[idx]+(diff_idx[idx+1]-diff_idx[idx])) if idx < len(diff_idx)-1 else (start_idx[idx]-1,end_idx[-1]) for idx in range(len(diff_idx))]\n",
    "\n",
    "# #     tickers = [df['text'][i][tup[0]:tup[1]] for tup in indexes]\n",
    "# #     comp_list.append(tickers)\n",
    "\n",
    "# comp_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cdfd8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import json\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from transformers import BertModel, TFBertModel, TFRobertaModel, RobertaTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoModelForSequenceClassification\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split,KFold,StratifiedKFold\n",
    "from adabelief_pytorch import AdaBelief\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import shutil\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "920c44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.cuda.amp as amp\n",
    "import os\n",
    "from transformers import XLMPreTrainedModel, XLMRobertaModel, XLMRobertaConfig, XLMRobertaTokenizer\n",
    "from transformers import XLMRobertaForSequenceClassification, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertForSequenceClassification, DistilBertForSequenceClassification, XLNetForSequenceClassification,\\\n",
    "XLMRobertaForSequenceClassification, XLMForSequenceClassification, RobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "#random seed 고정\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "\n",
    "# class args\n",
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    debug=False\n",
    "    amp = True\n",
    "    gpu = '0'\n",
    "    \n",
    "    batch_size=32\n",
    "    max_len = 128\n",
    "    \n",
    "    start_lr = 1e-5#1e-3,5e-5\n",
    "    min_lr=1e-6\n",
    "    # ---- Dataset ---- #\n",
    "\n",
    "    # ---- Else ---- #\n",
    "    num_workers=8\n",
    "    seed=2021\n",
    "    scheduler = None#'get_linear_schedule_with_warmup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "935c8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\", cache_dir='bert_ckpt', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c057c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df_data = df\n",
    "    def __getitem__(self, index):\n",
    "        # get the sentence from the dataframe\n",
    "        sentence = self.df_data.loc[index, 'text']\n",
    "        encoded_dict = tokenizer(\n",
    "          text = sentence,\n",
    "          add_special_tokens = True, \n",
    "          max_length = args.max_len,\n",
    "          pad_to_max_length = True,\n",
    "          truncation=True,           # Pad & truncate all sentences.\n",
    "          return_tensors=\"pt\")\n",
    "\n",
    "        padded_token_list = encoded_dict['input_ids'][0]\n",
    "        token_type_id = encoded_dict['token_type_ids'][0]\n",
    "        att_mask = encoded_dict['attention_mask'][0]\n",
    "        sample = (padded_token_list, token_type_id , att_mask)\n",
    "        return sample\n",
    "    def __len__(self):\n",
    "        return len(self.df_data)\n",
    "\n",
    "test_data = TestDataset(df)\n",
    "    \n",
    "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        shuffle=False,\n",
    "                                      num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f4e1c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def do_predict(net, valid_loader):\n",
    "    \n",
    "    val_loss = 0\n",
    "    pred_lst = []\n",
    "    logit=[]\n",
    "    net.eval()\n",
    "    for batch_id, (input_id,token_type_id,attention_mask) in enumerate(tqdm_notebook(valid_loader)):\n",
    "        input_id = input_id.long().to(device)\n",
    "        token_type_id = token_type_id.long().to(device)\n",
    "        attention_mask = attention_mask.long().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if args.amp:\n",
    "                with amp.autocast():\n",
    "                    # output\n",
    "                    output = net(input_ids=input_id, token_type_ids=token_type_id, attention_mask=attention_mask)[0]\n",
    "\n",
    "            else:\n",
    "                output = net(outputs = model(input_ids=input_id, token_type_ids=token_type_id, attention_mask=attention_mask))\n",
    "             \n",
    "            pred_lst.extend(output.argmax(dim=1).tolist())\n",
    "            logit.extend(output.tolist())\n",
    "            \n",
    "    return pred_lst,logit\n",
    "\n",
    "def run_predict(model_path,test_dataloader):\n",
    "\n",
    "\n",
    "    print('set testloader')\n",
    "    ## net ----------------------------------------\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    net = RobertaForSequenceClassification.from_pretrained('klue/roberta-base', num_labels = 3) \n",
    "\n",
    "        \n",
    "    net.to(device)\n",
    "    \n",
    "    if len(args.gpu)>1:\n",
    "        net = nn.DataParallel(net)\n",
    "\n",
    "    f = torch.load(model_path)\n",
    "    net.load_state_dict(f, strict=True)  # True\n",
    "    print('load saved models')\n",
    "    # ------------------------\n",
    "    # validation\n",
    "    preds, logit = do_predict(net, test_dataloader) #outputs\n",
    "           \n",
    "    print('complete predict')\n",
    "    \n",
    "    return preds, np.array(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cd98c6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_15640\\2994675733.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (input_id,token_type_id,attention_mask) in enumerate(tqdm_notebook(valid_loader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a56ff12866a42e2bb190d83dbcec062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82103\\anaconda3\\envs\\DL\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n"
     ]
    }
   ],
   "source": [
    "preds1, logit1 = run_predict('./model/klue_base_fold3_s.pth',test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0587025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['감성분류']=preds1\n",
    "df.감성분류=df.감성분류.apply(lambda x: '중립' if x == 0 else '긍정' if x == 1 else '부정')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e363249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_=df.loc[:,['text','tag','기업명','감성분류']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "adb2dacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>기업명</th>\n",
       "      <th>감성분류</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[아시아경제 손선희 기자] 지난해 부진한 실적을 기록한 현대백화점에 대해 증권사들이 줄줄이 목표주가를 낮춰 잡았다.</td>\n",
       "      <td>[O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>아시아경제,현대백화점</td>\n",
       "      <td>부정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>한국투자증권은 14일 현대백화점에 대한 목표주가를 기존 9만원에서 8만원으로 하향 조정한다고 밝혔다.</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>한국투자증권,현대백화점</td>\n",
       "      <td>부정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>현대백화점은 지난해 4분기 연결기준 영업이익 686억원을 기록했다.</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>현대백화점</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>전년 동기 대비 27.2% 줄었을 뿐만 아니라 시장 기대치에도 ..</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>부정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[헤럴드경제=증권부] 필옵틱스는 지난해 연결기준 영업이익이 176억9866만원으로 전년대비 흑자전환했다고 14일 공시했다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>필옵틱스</td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>같은 기간 매출액은 3039억8773만원으로 31.7% 증가한 것으로 나타났다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>당기순이익 역시 55억2147만원으로 흑자전환했다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jiyoon436@heraldcorp.com</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>펄어비스가 지난해 매출 3860억원, 영업이익 166억원, 당기순손실 411억원을 기록했다고 14일 밝혔다.</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>펄어비스</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>전년 대비 매출과 영업이익이 각각 4.4%, 61.4% 감소했다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>부정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4분기 매출은 1032억원, 영업이익은 36억원이다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>펄어비스는 '검은사막' IP(지식재산권)와 '이브'의 지속적인 콘텐츠 업데이트, 이용자 중심의 마케팅으로 글로벌 시..</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>펄어비스</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[아시아경제 박선미 기자]한달 뒤 열리는 삼성전자 정기 주주총회에서 이재용 삼성전자 회장의 등기이사 선임이 안건에 포함될지 여부에 관심이 쏠리고 있다.</td>\n",
       "      <td>[O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>아시아경제 박선미 기자]한달 뒤 열리는 삼성전자</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14일 삼성전자는 오전에 열리는 이사회에서 주총 소집일과 안건 등을 결정한다.</td>\n",
       "      <td>[O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>삼성전자</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>주총 소집일은 다음달 15일 전후가 될 가능성이 큰 상황이다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>이사회 결정에 따라 주총 안건은 이번주 안에 공시될 예정..</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[헤럴드경제=정찬수 기자] 현대자동차와 기아가 지난달 인도에서 월간 최다 판매기록을 다시 썼다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>현대자동차,기아</td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SUV(스포츠유틸리티차)와 MPV(다목적차량)에 이어 신흥시장에서 관심이 높은 전기차 시장 점유율 확대 전략에도 탄력이 예상된다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14일 인도자동차공업협회(SIAM)와 현대차그룹에 따르면 현대차는 지난달 작년 같은 기간보다 13.8% 증가한 5만106..</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, O, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>현대차그룹</td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[아시아경제 정동훈 기자] 나경수 SK지오센트릭 사장이 글로벌 기후위기, 폐플라스틱 문제 등에 선제적으로 대응해 재활용 분야 글로벌 선두 기업으로 자리매김하겠다는 각오를 밝혔다.</td>\n",
       "      <td>[O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...]</td>\n",
       "      <td>아시아경제,SK지오센트릭</td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14일 나 사장은 최근 SK이노베이션 계열의 보도채널 '스키노뉴스'와 진행한 인터뷰에서 \"SK지오센트릭은 세계 최고의 리사이클링 소재 기업으로의 전환을 선언한 만큼 더 빠..</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>SK이노베이션,SK지오센트릭</td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>신영 증권입니다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>뉴욕 증시가 '소비자 물가지수' 발표를 앞두고 상승 마감했습니다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>S&amp;P500 지수는 1.1%, 나스닥 지수는 1.4% 각각 상승했는데요.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>우리 시간으로 오늘 밤에 발표될,, 미국의 1월 '소비자 물가지수' 상승률이, 지난달과 대비해 둔화될 것이라는 기대가 큰 상황 속에 '투자심리'가 유지됐다는 분석입니다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>부정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>시장은 1월 '소비자..</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>◀ 앵커 ▶\\n\\n출생률이 급격히 떨어지면서 정원 미달로 문을 닫는 어린이집이 급증하고 있습니다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>부정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>하지만 반대로, 일부 신도시에선 아이 보낼 곳을 찾지 못해 부모들이 애를 태우고 있는데요.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>부정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>김민형 기자가 취재했습니다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>◀ 리포트 ▶\\n\\n3년 전 정원 60명을 모두 채웠던 이 어린이집의 원생은 올해 절반으로 줄었습니다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>부정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>특히 지난해 태어난 1세 아이..</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>삼성전자서비스가 뛰어난 수리 기술과 친절한 서비스로 고객에게 가장 좋은 평가를 받은 AS 엔지니어 18명을 '2022년 CS 달인'으로 선정했다고 14일 밝혔다.</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>삼성전자</td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>CS 달인은 서비스 제공 후 진행되는 고객 만족도 설문에서 최고의 평가를 받은 엔지니어를 선발하는 제도로 2019년부터 시행 중이다.</td>\n",
       "      <td>[O, O, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2022년 CS 달인은 AS 엔지니어 5800명..</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>KB증권은 14일 LG전자에 대해 연말 LG전자 전장(VS) 수주잔고가 100조원에 근접할 것이라며, 주가 재평가가 기대된다고 진단했다.</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>KB증권</td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>김동원 KB증권 연구원은 \"전일 LG전자에 따르면 작년 전장 사업 매출이 처음으로 전체 매출의 10.4%를 차지하며 최대 매출을 기록했다\"며 \"올해 회사의 VS 부문 매출액은 전년 대비 25% 증가한 10조800..</td>\n",
       "      <td>[O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...]</td>\n",
       "      <td>KB증권,LG전자</td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>KFC는 간장 치킨 메뉴 ‘뉴갓쏘이치킨’을 출시했다고 14일 밝혔다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>뉴갓쏘이치킨은 2020년 출시한 갓쏘이치킨의 업그레이드 버전으로, 육즙 가득 바삭한 치킨에 KFC의 비법 간장 소스의 감칠맛이 더해졌다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>한국인의 입맛에 맞춘 양념에 중독적인 단짠의 맛을 느낄 수 있어 간장치킨을 좋아하는 고객은 물론 대중적인 맛을 선호하는 고객도 즐길 수 있다...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[KBS 전주]\\n[게시판] 익산시 다이로움 취업박람회 외</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>가전의 신세계 ‘에어오븐’ 열풍 \\n보급률 60% 돌파한 필수가전 \\n오븐형 신제품으로 세대교체 \\n스테이크 베이킹 등 활용도 높아\\n\\n높아진 물가로 외식 대신 집에서 식사를 하는 인구가 늘어나고 있다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...]</td>\n",
       "      <td></td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>특히 에어프라이어의 경우 요리를 쉽게 조리할 수 있는 ‘간편 요리가전’으로 불리며 주방 필수 가전으로 자리잡았다.</td>\n",
       "      <td>[O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>에어프라이어</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>에어프라이어만 사용하면 완성되는 냉동식품과 ..</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>에어프라이어</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[헤럴드경제=김지헌 김민지 기자] “(반도체 공장 착공 지연의) 도가 지나칩니다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, O, O, O, O]</td>\n",
       "      <td>지나칩</td>\n",
       "      <td>부정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>조속히 반도체 공장을 지을 수 있도록 정부가 특단의 조치를 취해야 합니다.”(반도체 업계 고위 관계자) \\n \\n 오는 22일 정부가 용인 반도체 클러스터 사업 착수를 발표된 지 만 4년이 된다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>그러나 SK하이닉스 반도체 공장은 ‘첫 삽’조차 뜨지 못하면서 관련 산업계 우려..</td>\n",
       "      <td>[O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>SK하이닉스</td>\n",
       "      <td>부정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>롯데 유통군이 지진 피해를 입은 튀르키예와 시리아의 복구 지원에 나선다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>롯데 유통군은 튀르키예와 시리아 주민들을 위해 구호물품을 긴급 지원한다고 14일 밝혔다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>구호물품은 튀르키예 대사관에서 요청한 방한용 의류와 핫팩, 치약 칫솔 등 생필품을 중심으로 구성했으며, 구세군을 통해 지진 피해 현지로 전달할 예정이다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>이번 튀르키예 지진의 ..</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>부정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[KBS 전주]\\n전북은행은 금리 상승에 따른 순이자 이윤 확대 등에 따라 지난해 순이익이 2천76억 원으로 한 해 전보다 13.5퍼센트 증가했다고 밝혔습니다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>이자 이익은 5천9백52억 원으로 전해보다 20.8퍼센트 늘었고, 비이자이익은 2백55억 원 손실을 기록했습니다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>비용 효율성을 따지는 이익 경비율은 40.7퍼센트로 전해보다 7.1퍼센트 포인..</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>미국에서 여성 속옷에 부과되는 관세율이 남성 속옷보다 높다는 연구 결과가 제시됐다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>부정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>이는 대부분의 국가에서 여성용과 남성용 속옷에 같은 관세율이 적용되는 것과 다른 이례적 현상으로 풀이된다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>CNN은 \"핑크 택스에 해당하는 전형적인 예시\"라고 지적했다.</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>CNN에 따르면 미국에서 여성 속옷에 대한 평균 관세율은 15.5%로 남성 속옷 11.5%에 비..</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td></td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>[머니투데이 박수현 기자] 키움증권이 CJ제일제당에 대해 투자의견 '매수'를 유지하고 목표주가를 기존 50만원에서 48만원으로 하향 조정했다.</td>\n",
       "      <td>[O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>머니투데,키움증권,CJ제일제당</td>\n",
       "      <td>부정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>박상준 키움증권 연구원은 \"CJ제일제당의 지난해 4분기 연결기준 영업이익은 전년 동기 대비 2% 오른 2406억원으로 시장 기대치를 하회했다\"며 \"해외 식품 실적 호조에도 불구하고 식품과 F&amp;C 부문의 원재료 ..</td>\n",
       "      <td>[O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...]</td>\n",
       "      <td>키움증권,CJ제일제당</td>\n",
       "      <td>부정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>펄어비스(263750)는 연결 기준 지난해 연간 매출이 3860억원, 영업이익은 166억원을 기록했다고 14일 밝혔다.</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>펄어비스</td>\n",
       "      <td>긍정</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        text  \\\n",
       "0                                                           [아시아경제 손선희 기자] 지난해 부진한 실적을 기록한 현대백화점에 대해 증권사들이 줄줄이 목표주가를 낮춰 잡았다.   \n",
       "1                                                                   한국투자증권은 14일 현대백화점에 대한 목표주가를 기존 9만원에서 8만원으로 하향 조정한다고 밝혔다.   \n",
       "2                                                                                      현대백화점은 지난해 4분기 연결기준 영업이익 686억원을 기록했다.   \n",
       "3                                                                                      전년 동기 대비 27.2% 줄었을 뿐만 아니라 시장 기대치에도 ..   \n",
       "4                                                       [헤럴드경제=증권부] 필옵틱스는 지난해 연결기준 영업이익이 176억9866만원으로 전년대비 흑자전환했다고 14일 공시했다.   \n",
       "5                                                                               같은 기간 매출액은 3039억8773만원으로 31.7% 증가한 것으로 나타났다.   \n",
       "6                                                                                               당기순이익 역시 55억2147만원으로 흑자전환했다.   \n",
       "7                                                                                                   jiyoon436@heraldcorp.com   \n",
       "8                                                               펄어비스가 지난해 매출 3860억원, 영업이익 166억원, 당기순손실 411억원을 기록했다고 14일 밝혔다.   \n",
       "9                                                                                       전년 대비 매출과 영업이익이 각각 4.4%, 61.4% 감소했다.   \n",
       "10                                                                                             4분기 매출은 1032억원, 영업이익은 36억원이다.   \n",
       "11                                                        펄어비스는 '검은사막' IP(지식재산권)와 '이브'의 지속적인 콘텐츠 업데이트, 이용자 중심의 마케팅으로 글로벌 시..   \n",
       "12                                      [아시아경제 박선미 기자]한달 뒤 열리는 삼성전자 정기 주주총회에서 이재용 삼성전자 회장의 등기이사 선임이 안건에 포함될지 여부에 관심이 쏠리고 있다.   \n",
       "13                                                                               14일 삼성전자는 오전에 열리는 이사회에서 주총 소집일과 안건 등을 결정한다.   \n",
       "14                                                                                        주총 소집일은 다음달 15일 전후가 될 가능성이 큰 상황이다.   \n",
       "15                                                                                         이사회 결정에 따라 주총 안건은 이번주 안에 공시될 예정..   \n",
       "16                                                                     [헤럴드경제=정찬수 기자] 현대자동차와 기아가 지난달 인도에서 월간 최다 판매기록을 다시 썼다.   \n",
       "17                                                  SUV(스포츠유틸리티차)와 MPV(다목적차량)에 이어 신흥시장에서 관심이 높은 전기차 시장 점유율 확대 전략에도 탄력이 예상된다.   \n",
       "18                                                     14일 인도자동차공업협회(SIAM)와 현대차그룹에 따르면 현대차는 지난달 작년 같은 기간보다 13.8% 증가한 5만106..   \n",
       "19                       [아시아경제 정동훈 기자] 나경수 SK지오센트릭 사장이 글로벌 기후위기, 폐플라스틱 문제 등에 선제적으로 대응해 재활용 분야 글로벌 선두 기업으로 자리매김하겠다는 각오를 밝혔다.   \n",
       "20                         14일 나 사장은 최근 SK이노베이션 계열의 보도채널 '스키노뉴스'와 진행한 인터뷰에서 \"SK지오센트릭은 세계 최고의 리사이클링 소재 기업으로의 전환을 선언한 만큼 더 빠..   \n",
       "21                                                                                                                 신영 증권입니다.   \n",
       "22                                                                                      뉴욕 증시가 '소비자 물가지수' 발표를 앞두고 상승 마감했습니다.   \n",
       "23                                                                                  S&P500 지수는 1.1%, 나스닥 지수는 1.4% 각각 상승했는데요.   \n",
       "24                            우리 시간으로 오늘 밤에 발표될,, 미국의 1월 '소비자 물가지수' 상승률이, 지난달과 대비해 둔화될 것이라는 기대가 큰 상황 속에 '투자심리'가 유지됐다는 분석입니다.   \n",
       "25                                                                                                             시장은 1월 '소비자..   \n",
       "26                                                                    ◀ 앵커 ▶\\n\\n출생률이 급격히 떨어지면서 정원 미달로 문을 닫는 어린이집이 급증하고 있습니다.   \n",
       "27                                                                        하지만 반대로, 일부 신도시에선 아이 보낼 곳을 찾지 못해 부모들이 애를 태우고 있는데요.   \n",
       "28                                                                                                           김민형 기자가 취재했습니다.   \n",
       "29                                                                 ◀ 리포트 ▶\\n\\n3년 전 정원 60명을 모두 채웠던 이 어린이집의 원생은 올해 절반으로 줄었습니다.   \n",
       "30                                                                                                        특히 지난해 태어난 1세 아이..   \n",
       "31                                삼성전자서비스가 뛰어난 수리 기술과 친절한 서비스로 고객에게 가장 좋은 평가를 받은 AS 엔지니어 18명을 '2022년 CS 달인'으로 선정했다고 14일 밝혔다.   \n",
       "32                                                CS 달인은 서비스 제공 후 진행되는 고객 만족도 설문에서 최고의 평가를 받은 엔지니어를 선발하는 제도로 2019년부터 시행 중이다.   \n",
       "33                                                                                              2022년 CS 달인은 AS 엔지니어 5800명..   \n",
       "34                                              KB증권은 14일 LG전자에 대해 연말 LG전자 전장(VS) 수주잔고가 100조원에 근접할 것이라며, 주가 재평가가 기대된다고 진단했다.   \n",
       "35  김동원 KB증권 연구원은 \"전일 LG전자에 따르면 작년 전장 사업 매출이 처음으로 전체 매출의 10.4%를 차지하며 최대 매출을 기록했다\"며 \"올해 회사의 VS 부문 매출액은 전년 대비 25% 증가한 10조800..   \n",
       "36                                                                                    KFC는 간장 치킨 메뉴 ‘뉴갓쏘이치킨’을 출시했다고 14일 밝혔다.   \n",
       "37                                              뉴갓쏘이치킨은 2020년 출시한 갓쏘이치킨의 업그레이드 버전으로, 육즙 가득 바삭한 치킨에 KFC의 비법 간장 소스의 감칠맛이 더해졌다.   \n",
       "38                                         한국인의 입맛에 맞춘 양념에 중독적인 단짠의 맛을 느낄 수 있어 간장치킨을 좋아하는 고객은 물론 대중적인 맛을 선호하는 고객도 즐길 수 있다...   \n",
       "39                                                                                          [KBS 전주]\\n[게시판] 익산시 다이로움 취업박람회 외   \n",
       "40        가전의 신세계 ‘에어오븐’ 열풍 \\n보급률 60% 돌파한 필수가전 \\n오븐형 신제품으로 세대교체 \\n스테이크 베이킹 등 활용도 높아\\n\\n높아진 물가로 외식 대신 집에서 식사를 하는 인구가 늘어나고 있다.   \n",
       "41                                                           특히 에어프라이어의 경우 요리를 쉽게 조리할 수 있는 ‘간편 요리가전’으로 불리며 주방 필수 가전으로 자리잡았다.   \n",
       "42                                                                                                에어프라이어만 사용하면 완성되는 냉동식품과 ..   \n",
       "43                                                                             [헤럴드경제=김지헌 김민지 기자] “(반도체 공장 착공 지연의) 도가 지나칩니다.   \n",
       "44             조속히 반도체 공장을 지을 수 있도록 정부가 특단의 조치를 취해야 합니다.”(반도체 업계 고위 관계자) \\n \\n 오는 22일 정부가 용인 반도체 클러스터 사업 착수를 발표된 지 만 4년이 된다.   \n",
       "45                                                                            그러나 SK하이닉스 반도체 공장은 ‘첫 삽’조차 뜨지 못하면서 관련 산업계 우려..   \n",
       "46                                                                                  롯데 유통군이 지진 피해를 입은 튀르키예와 시리아의 복구 지원에 나선다.   \n",
       "47                                                                         롯데 유통군은 튀르키예와 시리아 주민들을 위해 구호물품을 긴급 지원한다고 14일 밝혔다.   \n",
       "48                                     구호물품은 튀르키예 대사관에서 요청한 방한용 의류와 핫팩, 치약 칫솔 등 생필품을 중심으로 구성했으며, 구세군을 통해 지진 피해 현지로 전달할 예정이다.   \n",
       "49                                                                                                            이번 튀르키예 지진의 ..   \n",
       "50                                 [KBS 전주]\\n전북은행은 금리 상승에 따른 순이자 이윤 확대 등에 따라 지난해 순이익이 2천76억 원으로 한 해 전보다 13.5퍼센트 증가했다고 밝혔습니다.   \n",
       "51                                                           이자 이익은 5천9백52억 원으로 전해보다 20.8퍼센트 늘었고, 비이자이익은 2백55억 원 손실을 기록했습니다.   \n",
       "52                                                                             비용 효율성을 따지는 이익 경비율은 40.7퍼센트로 전해보다 7.1퍼센트 포인..   \n",
       "53                                                                            미국에서 여성 속옷에 부과되는 관세율이 남성 속옷보다 높다는 연구 결과가 제시됐다.   \n",
       "54                                                               이는 대부분의 국가에서 여성용과 남성용 속옷에 같은 관세율이 적용되는 것과 다른 이례적 현상으로 풀이된다.   \n",
       "55                                                                                        CNN은 \"핑크 택스에 해당하는 전형적인 예시\"라고 지적했다.   \n",
       "56                                                                   CNN에 따르면 미국에서 여성 속옷에 대한 평균 관세율은 15.5%로 남성 속옷 11.5%에 비..   \n",
       "57                                           [머니투데이 박수현 기자] 키움증권이 CJ제일제당에 대해 투자의견 '매수'를 유지하고 목표주가를 기존 50만원에서 48만원으로 하향 조정했다.   \n",
       "58   박상준 키움증권 연구원은 \"CJ제일제당의 지난해 4분기 연결기준 영업이익은 전년 동기 대비 2% 오른 2406억원으로 시장 기대치를 하회했다\"며 \"해외 식품 실적 호조에도 불구하고 식품과 F&C 부문의 원재료 ..   \n",
       "59                                                        펄어비스(263750)는 연결 기준 지난해 연간 매출이 3860억원, 영업이익은 166억원을 기록했다고 14일 밝혔다.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                  tag  \\\n",
       "0                                                                                                                      [O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1                                                                                                                                          [O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "2                                                                                                                                                                                                                           [O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                                                                                                                                                                                                                                               [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4                                                                                                                                  [O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "5                                                                                                                                                                                                                          [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "6                                                                                                                                                                                                                                                                          [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "7                                                                                                                                                                                                                                                                                      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "8                                                                                                                                                          [O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "9                                                                                                                                                                                                                                                  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "10                                                                                                                                                                                                                                                                      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "11                                                                                                                                       [O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "12                                                                         [O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "13                                                                                                                                                                                                            [O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "14                                                                                                                                                                                                                                                       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "15                                                                                                                                                                                                                                                          [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "16                                                                                                                                                                  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "17                                                                                                                                 [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "18                                                                                                                              [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, O, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "19  [O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...]   \n",
       "20  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "21                                                                                                                                                                                                                                                                                                                                  [O, O, O, O, O, O, O, O, O, O, O]   \n",
       "22                                                                                                                                                                                                                                                 [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "23                                                                                                                                                                                                                                     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "24                                                                   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "25                                                                                                                                                                                                                                                                                                                      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "26                                                                                                                                                                                                 [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "27                                                                                                                                                                                                       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "28                                                                                                                                                                                                                                                                                                                [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "29                                                                                                                                                                                        [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "30                                                                                                                                                                                                                                                                                                       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "31                                                               [O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "32                                                                                                                           [O, O, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "33                                                                                                                                                                                                                                                                         [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "34                                                                                                         [O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "35                  [O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...]   \n",
       "36                                                                                                                                                                                                                                           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "37                                                                                                                         [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "38                                                                                                          [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "39                                                                                                                                                                                                                                                                [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "40                                                  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...]   \n",
       "41                                                                                                                                        [O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "42                                                                                                                                                                                                                                                       [O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "43                                                                                                                                                                                                          [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, O, O, O, O]   \n",
       "44                                                  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...]   \n",
       "45                                                                                                                                                                                           [O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "46                                                                                                                                                                                                                                     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "47                                                                                                                                                                                                          [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "48                                                                                              [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "49                                                                                                                                                                                                                                                                                                                   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "50                                                                                     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "51                                                                                                                                                                [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "52                                                                                                                                                                                                                      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "53                                                                                                                                                                                                                   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "54                                                                                                                                                                            [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "55                                                                                                                                                                                                                                                       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "56                                                                                                                                                                                        [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "57                                                        [O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "58          [O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...]   \n",
       "59                                                                                                                                       [O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                           기업명 감성분류  \n",
       "0                  아시아경제,현대백화점   부정  \n",
       "1                 한국투자증권,현대백화점   부정  \n",
       "2                        현대백화점   중립  \n",
       "3                                부정  \n",
       "4                         필옵틱스   긍정  \n",
       "5                                긍정  \n",
       "6                                긍정  \n",
       "7                                중립  \n",
       "8                         펄어비스   중립  \n",
       "9                                부정  \n",
       "10                               중립  \n",
       "11                        펄어비스   중립  \n",
       "12  아시아경제 박선미 기자]한달 뒤 열리는 삼성전자   중립  \n",
       "13                        삼성전자   중립  \n",
       "14                               중립  \n",
       "15                               중립  \n",
       "16                    현대자동차,기아   긍정  \n",
       "17                               긍정  \n",
       "18                       현대차그룹   긍정  \n",
       "19               아시아경제,SK지오센트릭   긍정  \n",
       "20             SK이노베이션,SK지오센트릭   긍정  \n",
       "21                               중립  \n",
       "22                               긍정  \n",
       "23                               긍정  \n",
       "24                               부정  \n",
       "25                               중립  \n",
       "26                               부정  \n",
       "27                               부정  \n",
       "28                               중립  \n",
       "29                               부정  \n",
       "30                               중립  \n",
       "31                        삼성전자   긍정  \n",
       "32                               중립  \n",
       "33                               중립  \n",
       "34                        KB증권   긍정  \n",
       "35                   KB증권,LG전자   긍정  \n",
       "36                               중립  \n",
       "37                               중립  \n",
       "38                               중립  \n",
       "39                               중립  \n",
       "40                               긍정  \n",
       "41                      에어프라이어   중립  \n",
       "42                      에어프라이어   중립  \n",
       "43                         지나칩   부정  \n",
       "44                               중립  \n",
       "45                      SK하이닉스   부정  \n",
       "46                               중립  \n",
       "47                               중립  \n",
       "48                               중립  \n",
       "49                               부정  \n",
       "50                               긍정  \n",
       "51                               긍정  \n",
       "52                               긍정  \n",
       "53                               부정  \n",
       "54                               중립  \n",
       "55                               중립  \n",
       "56                               중립  \n",
       "57            머니투데,키움증권,CJ제일제당   부정  \n",
       "58                 키움증권,CJ제일제당   부정  \n",
       "59                        펄어비스   긍정  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8f11d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_.to_csv('test_ner_sentiment.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc2e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
